{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fogler P3-10C. Calculating Arrhenius Parameters\n",
    "## An exploration of different methods for fitting data to known functional forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard start to almost all our Python code...\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the raw data from the textbook\n",
    "T_data = np.array([300.0, 320.0, 340.0, 360.0]) \n",
    "rate = np.array([0.002, 0.046, 0.72, 8.33])\n",
    "A = 2.0 #M\n",
    "B = 1.5 #M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are told that the reaction is:\n",
    "\n",
    "$$2A + B \\rightarrow 4C$$\n",
    "\n",
    "it is also (apparently) an **elementary** reaction, so the rate law must be:\n",
    "\n",
    "$$r_A = -k[A]^2[B]$$\n",
    "\n",
    "We use this to calculate k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_data = rate/(A**2 * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from Arrhenius plot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the Arrhenius relationship, we plot ln K vs 1/T \n",
    "X = 1/T_data\n",
    "Y = np.log(k_data) #This is how we do natural logs in numpy\n",
    "plt.scatter(X,Y)\n",
    "plt.ylabel(r'$ln k$')\n",
    "plt.xlabel(r'$\\frac{1}{T}$') #LaTeX-like math expressions using Matplotlib's built-in mathtext environment\n",
    "plt.xlim([min(X), max(X)])\n",
    "plt.title('Arrhenius Plot for P3-10C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the linear transformation, we can simply calculate a line of best fit\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_error = linregress(X,Y)\n",
    "X_plot = np.linspace(min(X),max(X),10)\n",
    "Y_plot = slope*X_plot+intercept\n",
    "plt.plot(X_plot,Y_plot, 'k')\n",
    "plt.scatter(X,Y)\n",
    "plt.ylabel(r'$ln k$')\n",
    "plt.xlabel(r'$\\frac{1}{T}$') #LaTeX-like math expressions using Matplotlib's built-in mathtext environment\n",
    "plt.xlim([min(X), max(X)])\n",
    "plt.title('Data Fitting from Arrhenius Plot')\n",
    "print('r^2= ',r_value**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now extract the activation energy from the slope of the plot\n",
    "R = 8.314 #J/mol-K\n",
    "E_act = slope * R / -1000.0\n",
    "A = np.exp(intercept)\n",
    "print('A = ', A)\n",
    "print('E_act = ', E_act, ' kJ/mol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear regression\n",
    "## A better way (maybe)!\n",
    "Since we're no longer using rulers to draw straight lines, there's no reason to transform the data before fitting it. We can use non-linear regression to get there instead. We will use the curve_fit function from scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First define a function that takes T data, and spits out guesses for k, based on a parsed value for E_a and A\n",
    "#NOTICE that the first argument to the function MUST be the independent data (e.g. x)\n",
    "def arrhenius(T, E_a, A):\n",
    "    k = A*np.exp(-1.0* E_a / (R * T))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the least-squares method, curve_fit, to fit this function to the data we have. Mathematically, what's going on is the minimization of error between predicted values (calculated from a function we define) and the actual experimental data. If $\\theta$ is the parameters to optimize, then $$\\hat{\\theta} = \\underset{\\theta}{\\operatorname{argmin}} \\sum_{i=1}^{n}\\left(f_{\\theta}(x_i)-y_i\\right)^2 $$\n",
    "\n",
    "Curve_fit takes a minimum of three arguments: 1) the function to optimize (in this case arrhenius), 2) the x data, 3) the y data. We will also be providing an optional fourth arugment, which represents an initial guess for the two parameters.\n",
    "\n",
    "The curve_fit function has two outputs. The first, here popt is an array of the fitted parameters, in the order listed in the function being optimized. The second pcov describes some of the statistics around the fit, which we'll ignore for now but which will become useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(arrhenius, T_data, k_data, [124000, 1.7E18])\n",
    "E_act_nln = popt[0] / 1000.0\n",
    "A_nln = popt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A = ', A_nln)\n",
    "print('E_act = ', E_act_nln, ' kJ/mol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one should I use?\n",
    "\n",
    "Notice that in this example, the results are almost identical. This will not always be the case! In general, here are some things to consider when decidinng which approach to use:\n",
    "\n",
    "* Transforming data into linear form can introduce error (especially when taking reciprocals of small numbers). We'll see examples of this when we study enzyme systems later in the course\n",
    "* Using curve_fit requires an optimization algorithm. Generally speaking these solvers are great at finding **local** minima, but guaranteeting that you have found the **global** minimum is really hard. So, the quality of your parameters could depend on the initial guess you provide, especially for highly non-linear systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
